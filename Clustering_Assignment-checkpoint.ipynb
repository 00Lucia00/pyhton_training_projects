{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQbvlY1WxIl9"
   },
   "source": [
    "# Exercises of application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUG03eC8xIl-"
   },
   "source": [
    "You have a set of data on the customers of a shopping mall such as their gender, age, annual income and spending score. The objective of this assignment is to use the clustering methods outlined in the slides to determine which categories of customers have better spending scores. To download the data set click on the link <a href=\"https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python\">Mall Customers</a>  \n",
    "\n",
    "1. Make scatter plots of the different variables by differentiating each point in space by the color of the sex of the individual. You will use the `scatter()` function of matplotlib. \n",
    "2. The graphical results show that the formation of several groups according to certain variables. What are these variables ? \n",
    "3. Use the Elbow method in the k-means to find the optimal number of clusters. To do this, you need to determine the sums of the squares of the distances (total inertia) found in the set of methods of the `KMeans()` function. \n",
    "4. You will use the different metrics stated at the end of the exercise. You will use the `sklearn.metrics` module to import these evaluation measures and for similarity measures like the Euclidean distance you will use `sklearn.metrics.pairwise`\n",
    "5. For each of these methods, you will compare the results of each method with that of the k-means. \n",
    "\n",
    "before all thing, don't forget to make scale on the data (you will use the `StandardScaler()` imported for `sklearn.preprocessing`). \n",
    "\n",
    "For this, you will use the following clustering techniques:\n",
    "\n",
    "1. Partition-based clustering\n",
    "\n",
    "     * K-means  \n",
    "          - For this method, make the graphic also the clusters and centroids\n",
    "          - Determine the total inertia and the interpretation (The inertia represents the sum of the squares of the distances and is contained in the set of parameters of the KMeans method of the module `sklearn.cluster`)\n",
    "          - Evaluate this algorithm with the different unsupervised evaluation measures stated at the end of the exercise.\n",
    "          - Plot the silhouette graph with the `plt.barh()` function and Get the average silhouette score and plot it. For silhouette samples you will use the function `silhouette_samples` to apply to the standardized data and to the data the predicted values (`KMeans().fit_predict()`)\n",
    "          - interpret these results.\n",
    "\n",
    "     * K-medoids\n",
    "          - Apply this method on the data and make the graphical representation of clusters and medoids. You must install the `Kmedoids` module with the command `!pip install Kmedoids`, You will use the `method Kmedoids.pam()` on the dissimilarity matrix that you must calculate with the appropriate distance. specify the default parameters in your function.\n",
    "          - calculed intra-medoid and inter-medoid silhoutte coeficient. Use the `kmedoids.silhouette()` functions to determine the intramedoid silhouette index and `kmedoids.medoid_silhouette()` to determine the intermedoid silhouette index. \n",
    "          - Analysed the results \n",
    "\n",
    "     * Fuzzy C-means\n",
    "          - apply this method on the data and the cluster centers and the membership matrix. Use the `! pip install fuzzy-c-means` command to install the library, import the `FCM` function from the `fcmeans` package. Specify the number of ideal clusters, the number of iterations and the `random_state` and train the model on the standardized data. Obtain the cluster centers and the membership matrix from the `centers` function of the model. Assign each data point to the cluster with the highest membership with the constructor `u` of model (`model.fit().u`) to retrieve only the values. Assign each data point to the cluster with the highest membership using the function `np.armax()` which returns the indices of the maximum values along an axis.\n",
    "          - For this method, make the graphic also the clusters and centroids\n",
    "          - Evaluate this algorithm with the differents similarities measures unsupervised such as the partition coefficient and partition entropy coefficient \n",
    "          - determine the silhoutte scores, calinski harabasz score and davies bouldin score and interpreted interpret these results\n",
    "\n",
    "     * Spectral clustering (optional)\n",
    "          - determine the matrices of the non-normalized and normalized Laplacian\n",
    "          - use Reducing the dimensions of the data and given the principal components\n",
    "          - apply the spectral clustering on the data and visualy the clsuters \n",
    "          - analys each clusters and plot the validation indices as a function of the number of nearest neighbors\n",
    "          - plot Validation indices as a function of the number k of classes\n",
    "\n",
    "2. Hierarchical Clustering\n",
    "\n",
    "     * Agglomerative clustering\n",
    "          - Determine the dissimilarity matrix with the Ward distance and \n",
    "          - plot the dendrogram for the linkage_array containing the distances between clusters\n",
    "          -  represents the points that belong to the clusters and visualize the clusters\n",
    "          - determine the silhoutte scores, calinski harabasz score and davies bouldin score and interpreted interpret these results\n",
    "\n",
    "3. Density-based Clustering\n",
    "\n",
    "     * DBSCAN\n",
    "          - build the number of clusters in the labels, ignoring noise if it is present.\n",
    "          - make the graphic also the clusters and the number of noise \n",
    "          - give a graphical representation of the clusters and the number of noise with eps and min_samples\n",
    "          - determine the silhoutte scores, calinski harabasz score and davies bouldin score and interpreted interpret these results\n",
    "     \n",
    "\n",
    "**Unsupervised measures** : \n",
    " \n",
    "     - Silhouette Coefficient\n",
    "     - Calinski-Harabasz Index\n",
    "     - Davies-Bouldin Index    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SUBMISSION**"
   ],
   "metadata": {
    "id": "jKpArDA-xrDL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Put your work here ! \n",
    "https://drive.google.com/drive/folders/1WVltb3C61t5t-ZMQ12WfbUMNlN6U_Aih?usp=share_link"
   ],
   "metadata": {
    "id": "-EKjiwRYxUVt"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
