{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec5n97UA7oJF"
      },
      "source": [
        "### Part 1: Introduction to Keras and its features\n",
        "- Keras is a high-level API for building and training deep learning models\n",
        "- It is easy to use, flexible, and allows for rapid prototyping of models\n",
        "- Keras is built on top of TensorFlow library\n",
        "\n",
        "#### Keras History and Current Status\n",
        "- Keras was developed by Francois Chollet in 2015 as an open-source software library for building neural networks\n",
        "- In 2017, Keras was integrated into TensorFlow as the default API for building neural networks (tensorflow.keras)\n",
        "- As of 2021, Keras is widely used in the deep learning community and has a large and active developer community\n",
        "\n",
        "#### Features of Keras\n",
        "- Keras supports a variety of neural network architectures including feedforward, convolutional, and recurrent neural networks. It allows for easy customization of models through the use of layers, activation functions, and loss functions\n",
        "- Keras also includes a range of pre-built models and datasets for common tasks such as image classification and natural language processing\n",
        "\n",
        "#### Some of the key features of TensorFlow/Keras include:\n",
        "- Easy model building and prototyping using tensorflow.keras\n",
        "- Efficient computation on both CPUs and GPUs\n",
        "- Built-in support for distributed training\n",
        "- Large community with active development and support\n",
        "- Easy deployment of models on mobile and embedded devices\n",
        "\n",
        "#### Difference between TensorFlow/Keras\n",
        "TensorFlow and Keras are often used together to build deep learning models.\n",
        "1. TensorFlow is a low-level open-source library developed by Google Brain team that provides a wide range of tools for building and deploying machine learning models. It is highly customizable and allows for fine-grained control over the training process.\n",
        "2. Keras, on the other hand, is a high-level API that sits on top of TensorFlow. It provides a simpler interface for building and training deep learning models that are easy to use. Keras is designed for building models quickly and efficiently.\n",
        "\n",
        "The combination of those two libraries allows for rapid prototyping of deep learning models while still having access to the full power of TensorFlow. In this course, we will essentially be using Keras to build and train our models.\n",
        "\n",
        "### Part 2: Installation and setup\n",
        "TensorFlow/Keras can be installed using pip or conda. It is recommended to use a virtual environment to avoid conflicts with other libraries.\n",
        "\n",
        "1. Installing TensorFlow/Keras using pip or conda\n",
        "```python\n",
        "pip install tensorflow\n",
        "conda install tensorflow\n",
        "```\n",
        "\n",
        "2. In case you want to install TensorFlow for gpu, i recommend using the conda command\n",
        "```python\n",
        "conda install tensorflow-gpu\n",
        "```\n",
        "which will automatically install the gpu version of TensorFlow along with the necessary dependencies (such as CUDA toolkit and CUDNN).\n",
        "\n",
        "3. Verify version and GPU availability\n",
        "```python\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.test.gpu_device_name())\n",
        "```\n",
        "\n",
        "### Part 3: Introduction to Tensors and Variables in TensorFlow\n",
        "Tensors are the basic building blocks in TensorFlow/Keras, and can be thought of as multidimensional arrays or matrices. They are the data structures used to store and manipulate data in TensorFlow/Keras, and can be used to represent a wide range of data types such as images, audio, text, and numerical data.\n",
        "\n",
        "Variables, on the other hand, are a specific type of tensor that can be modified during the course of training. They are used to store the weights and biases of a model, and are updated during the training process using an optimization algorithm such as Stochastic Gradient Descent.\n",
        "\n",
        "In TensorFlow, tensors can be created using the tf.constant() method or the tf.Variable() method.\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a tensor with a constant value of 5\n",
        "a = tf.constant(5)\n",
        "print(a)\n",
        "```\n",
        "\n",
        "Output:\n",
        "```python\n",
        "tf.Tensor(5, shape=(), dtype=int32)\n",
        "```\n",
        "\n",
        "Creating a variable :\n",
        "```python\n",
        "# Create a variable with a constant value of 5\n",
        "b = tf.Variable(5)\n",
        "print(b)\n",
        "```\n",
        "\n",
        "Output:\n",
        "```python\n",
        "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>\n",
        "```\n",
        "\n",
        "It is also possible to create tensors from NumPy arrays using the tf.convert_to_tensor() method.\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Create a tensor from a NumPy array\n",
        "c = tf.convert_to_tensor(np.array([1, 2, 3]))\n",
        "print(c)\n",
        "```\n",
        "\n",
        "Output:\n",
        "```python\n",
        "tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n",
        "```\n",
        "\n",
        "#### Tensor Operations\n",
        "Similarly to NumPy or PyTorch libraries, TensorFlow/Keras supports a wide range of tensor operations such as addition, subtraction, multiplication, division, and matrix multiplication. These operations can be performed using the tf.add(), tf.subtract(), tf.multiply(), tf.divide(), and tf.matmul() methods respectively.\n",
        "\n",
        "```python\n",
        "# Create two tensors\n",
        "a = tf.constant(5)\n",
        "b = tf.constant(10)\n",
        "\n",
        "# Basic arithmetic operations in tensors\n",
        "c = tf.add(a, b)\n",
        "d = tf.subtract(a, b)\n",
        "e = tf.multiply(a, b)\n",
        "f = tf.divide(a, b)\n",
        "\n",
        "# Matrix multiplication of two tensors\n",
        "g = tf.matmul(a, b)\n",
        "```\n",
        "\n",
        "Other complex operations such as reshaping and concatenation can also be performed, refer to the PyTorch course, most of the operations are the same.\n",
        "\n",
        "```python\n",
        "# Convert a numpy array to a tensor\n",
        "numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "tensor_b = tf.convert_to_tensor(numpy_array)\n",
        "\n",
        "# Reshape a tensor\n",
        "tensor_c = tf.reshape(tensor_b, [3, 2])\n",
        "\n",
        "# Concatenate two tensors\n",
        "tensor_d = tf.concat([tensor_a, tensor_c], axis=0)\n",
        "```\n",
        "\n",
        "### Part 4: Gradients and Variable Updates\n",
        "Variables can be updated during training using Keras optimizer functions such as tf.keras.optimizers.Adam() and tf.keras.optimizers.SGD(). For example:\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Create a variable\n",
        "initial_value = np.random.randn(3, 4)\n",
        "variable_a = tf.Variable(initial_value)\n",
        "\n",
        "# Update the variable\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "with tf.GradientTape() as tape:\n",
        "    # Compute the loss\n",
        "    loss = some_function(variable_a)\n",
        "# Compute the gradients\n",
        "gradients = tape.gradient(loss, variable_a)\n",
        "# Update the variable\n",
        "optimizer.apply_gradients(zip(gradients, variable_a))\n",
        "```\n",
        "\n",
        "**Important note** The previous code may seem a bit confusing at first, however, we will not be using this method to update variables in this course. Instead, we will be using the Keras API to build and train our models. This method is only presented here for completeness. You should note that : \n",
        "- Keras allows training models with very fewer lines of code.\n",
        "- Keras allows the use of NumPy arrays as inputs to the model, so we don't have to convert them to tensors. But note it is always worth knowing how tensors work.\n",
        "\n",
        "#### Using Keras backend to access low-level operations\n",
        "Keras provides a backend module (keras.backend) that allows the user to access low-level TensorFlow operations. This is useful when creating custom layers or loss functions. One of the main reasons for using backend operations in Keras is to perform automatic differentiation for gradient backpropagation during training of neural networks.\n",
        "\n",
        "One example where it may be necessary to use the backend is when working with custom loss functions. For instance, let's say we have a custom loss function that requires a mathematical operation that is not directly supported by Keras. In this case, we can use the backend to perform the operation and still be able to compute gradients for backpropagation.\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "# Custom loss function using low-level TensorFlow operations\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return K.sum(tf.square(y_true - y_pred))\n",
        "```\n",
        "\n",
        "### Part 5: Building a feedforward neural network with dense layers\n",
        "To build a feedforward neural network with dense layers in Keras, we first need to import the necessary modules and functions:\n",
        "```python\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "```\n",
        "\n",
        "Then, we can define the model by creating a Sequential object and adding layers to it using the add method. Here's an example of a feedforward neural network with one hidden layer:\n",
        "```python\n",
        "# Define the model\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(784,)))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "```\n",
        "\n",
        "It is also possible to pass a whole array of layers to the Sequential constructor:\n",
        "```python\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "In this example, we define a model with two layers: a Dense layer with 64 units and ReLU activation function as the hidden layer, and a Dense layer with 10 units and softmax activation function as the output layer. The input_shape argument specifies the shape of the input data.\n",
        "- The Dense layer is equivalent to the Linear in PyTorch, it represents a fully connected layer.\n",
        "\n",
        "#### Activation functions\n",
        "Activation functions are essential components of a neural network because they allow the neural network to model more complex non-linear functions. Keras provides a variety of activation functions. For instance, we saw in the previous example the ReLU and softmax activation functions:\n",
        "- ReLU stands for Rectified Linear Unit, it is defined as ``max(x, 0)``. This function is well approriate in computer vision.\n",
        "- Softmax, defined as ``exp(x_i) / sum(exp(x))``, is an activation function that is used in the output layer of classification models. It is used to convert the output of the model to a probability distribution over the predicted output classes.\n",
        "\n",
        "In the previous example, we set the activation function by passing it as a string to the activation argument. However, we can also pass the activation function as a callable function. Keras provides multiple activation function in keras.activations, for example:\n",
        "```python\n",
        "model.add(layers.Dense(64, activation=keras.activations.relu))\n",
        "```\n",
        "\n",
        "For some activation functions, we can also use them as layers. For example, we can use the ReLU activation function as a layer by calling the keras.layers.ReLU() function. This allows to set some hyperparameters of the activation function:\n",
        "```python\n",
        "model.add(layers.Dense(64))\n",
        "model.add(layers.ReLU(alpha=0.01))\n",
        "```\n",
        "- A whole set of activations is provided by Keras, some of them can be specified by a simple string, such as \"tanh\" or \"sigmoid\", however for more complex functions, it is necessary to create the object first, and then pass it to the layer.\n",
        "**Note** When an activation is not specified, the layers will use the linear activation function by default.\n",
        "\n",
        "#### Compiling the model\n",
        "After defining the model, we need to compile it before training it. To compile the model, we need to specify the loss function, the optimizer, and the metrics to monitor during training. For example:\n",
        "```python\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "```\n",
        "\n",
        "Similar to the activation functions, we can also pass the loss function, optimizer, and metrics as callable functions:\n",
        "```python\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "```\n",
        "- Compiling the model will create the computation graph that will be used during training.\n",
        "- It is also possible to pass multiple metrics to the metrics argument, for example, metrics=['accuracy', 'mse'].\n",
        "- Similarly, if we have multiple outputs, we can pass multiple loss functions to the loss argument, for example, loss=['categorical_crossentropy', 'mse'] (We will see next how we can create models with multiple outputs and inputs).\n",
        "\n",
        "**Note** It also possible to print a summary of the model which will contain detailed informations about each layer and the number of parameters in the model. This is recommended to do after defining the model as a sanity check:\n",
        "```python\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "#### Training the model\n",
        "After compiling the model, we can train it using the fit method:\n",
        "```python\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val))\n",
        "```\n",
        "- The fit method takes the training data (x_train as inputs and y_train as targets), the batch size, the number of epochs, and the validation data if any. \n",
        "- It returns a History object that contains the training history of the model. \n",
        "- The history isa dictionnary containing the loss and metrics measured during training and validation.\n",
        "\n",
        "**Important note** Unlike PyTorch where we had to create DataLoaders for train and validation sets, in Keras, we can pass directly NumPy arrays containing our data. For instance, x_train and y_train are NumPy arrays, Keras takes care of turning them into tensors.\n",
        "\n",
        "#### Evaluating the model\n",
        "After training the model, we can evaluate it on the test set using the evaluate method:\n",
        "```python\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "```\n",
        "The evaluate function returns a list containing the loss and metrics values for the test set.\n",
        "\n",
        "#### Making predictions\n",
        "After training the model, we can use it to make predictions on new data using the predict method (single examples or batches of examples):\n",
        "```python\n",
        "predictions = model.predict(x_test)\n",
        "```\n",
        "``predictions`` will contains the outputs of the model, if the model has multiple outputs, then ``predictions`` will be a list.\n",
        "\n",
        "Regarding the previous example where the output was a probability distribution over the predicted output classes, we can get the predicted class by taking the argmax of the output:\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Get the class probabilities for the test images\n",
        "probs = model.predict(x_test)\n",
        "\n",
        "# Get the predicted class labels\n",
        "y_pred = np.argmax(probs, axis=1)\n",
        "\n",
        "# Print the first 10 predicted labels\n",
        "print(y_pred[:10])\n",
        "```\n",
        "\n",
        "### Part 6: Functional API\n",
        "- The Sequential API is very convenient for building simple models, however, it is limited in that it does not allow to create models with multiple inputs and outputs. For instance, if we want to create a model with two inputs and one output, we can't use the Sequential API. Instead, we need to use the Functional API.\n",
        "- Functional API offers more flexibility and freedom when constructing architectures.\n",
        "- Functional API consists on creating the different components of the model independently, and connecting them together. For instance, the previous example using Sequential can be rewritten using the Functional API as follows:\n",
        "```python\n",
        "# Define the input layer\n",
        "inputs = keras.Input(shape=(784,))\n",
        "# Define the hidden layer\n",
        "x = layers.Dense(64, activation='relu')(inputs)\n",
        "# Define the output layer\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "```\n",
        "\n",
        "- Note how each layer is connected to the previous one using the syntax ``x = layers_i(layer_{i-1})``. ``layer_i`` can be declared inplace directly or assigned to a variable. \n",
        "- At the end the keras.Model() function takes the list of inputs and outputs to create the model.\n",
        "- The final model has exactly the same functionalities as the Sequential model, we can compile it and train it the same way.\n",
        "\n",
        "We can also use Sequential models as part of functional models, for instance:\n",
        "```python\n",
        "# Define the input layer\n",
        "inputs = keras.Input(shape=(784,))\n",
        "\n",
        "# Define the hidden layer\n",
        "hiddens = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "])\n",
        "\n",
        "h = hiddens(inputs)\n",
        "\n",
        "# Define the output layer\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "```\n",
        "\n",
        "#### Multiple inputs and outputs (With multiple loss functions)\n",
        "- Functional API allows to create models with multiple inputs and outputs. For instance, if we want to create a model with two inputs and two outputs, we can do it as follows:\n",
        "```python\n",
        "# Define the first input\n",
        "inputs1 = keras.Input(shape=(784,))\n",
        "# Define the second input\n",
        "inputs2 = keras.Input(shape=(784,))\n",
        "# Concatenate the two inputs\n",
        "x = layers.concatenate([inputs1, inputs2])\n",
        "# Define the hidden layer\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "# Define the first output\n",
        "outputs1 = layers.Dense(10, activation='softmax')(x)\n",
        "# Define the second output\n",
        "outputs2 = layers.Dense(784, activation='sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model = keras.Model(inputs=[inputs1, inputs2], outputs=[outputs1, outputs2])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=['categorical_crossentropy', 'mse'],\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "```\n",
        "\n",
        "In this example, we have two inputs and two outputs. The first output is a classification output with 10 classes, and the second output is a reconstruction output with 784 pixels. We set 2 loss functions, one for each output. By default, Keras will optimize the sum of the two losses, however, we can also set different weights for each loss function using the loss_weights argument:\n",
        "```python\n",
        "model.compile(\n",
        "    loss=['categorical_crossentropy', 'mse'],\n",
        "    loss_weights=[1.0, 0.5],\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "```\n",
        "\n",
        "### Part 7: Custom layers\n",
        "- Keras allows to create custom layers by subclassing the Layer class. For instance, if we want to create a custom layer that adds a bias to the input, we can do it as follows:\n",
        "```python\n",
        "class BiasLayer(keras.layers.Layer):\n",
        "    def __init__(self, units=32, **kwargs):\n",
        "        super(BiasLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.units,),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.bias\n",
        "```\n",
        "\n",
        "- The BiasLayer class inherits from the Layer class and overrides the __init__, build and call methods.\n",
        "- The __init__ method is used to initialize the layer, it takes the layer parameters as arguments.\n",
        "- The build method is used to create the layer weights, it takes the input_shape as argument.\n",
        "- The call method is used to compute the layer output, it takes the inputs as argument (Equivalent to the forward() method in PyTorch).\n",
        "\n",
        "The BiasLayer class can be used as any other layer in Keras:\n",
        "```python\n",
        "# Create the model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    BiasLayer(64),\n",
        "    layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "```\n",
        "\n",
        "### Part 8: Custom loss functions\n",
        "- Keras allows to create custom loss functions by subclassing the Loss class. For instance, if we want to create a custom loss function that computes the mean squared error between the true and predicted outputs, we can do it as follows:\n",
        "```python\n",
        "class MSE(keras.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MSE, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "```\n",
        "\n",
        "- The MSE class inherits from the Loss class and overrides the __init__ and call methods.\n",
        "- The __init__ method is used to initialize internal parameters to the loss.\n",
        "- The call method is used to compute the loss, it takes the true and predicted outputs as arguments.\n",
        "**Note** call method always takes the true and predicted outputs as arguments, even if the loss function does not use the true outputs (e.g. in the case of a GAN).\n",
        "**Note2** Make sure to use the Keras backend functions to compute the loss, otherwise the loss will not be differentiable and there maybe a discontinuity in the computation graph.\n",
        "\n",
        "The new loss function can be used as any other loss function in Keras:\n",
        "```python\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=MSE(),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "```\n",
        "\n",
        "- Similarly, it is also possible to create custom metrics by subclassing the Metric class. However, we'll leave this part to the reader.\n",
        "\n",
        "### Part 9: Keras custom data generators\n",
        "- Keras allows to create custom data generators by subclassing the Sequence class. This can be helpful when loading and processing data on the fly, for instance when the data is too large to be loaded all at once in memory. Here's an example:\n",
        "```python\n",
        "class CustomDataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size=32):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.x) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        # you can preprocess your data at this point\n",
        "\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # you can shuffle your data or update the dataset at this point\n",
        "        # this function is optional\n",
        "        pass\n",
        "```\n",
        "\n",
        "- The CustomDataGenerator class inherits from the Sequence class and overrides the __init__, __len__, __getitem__ and on_epoch_end optional methods.\n",
        "- The __init__ method is used to initialize the data generator, it takes the data and batch_size as arguments.\n",
        "- The __len__ method is used to compute the number of batches in an epoch, it takes no arguments.\n",
        "- The __getitem__ method is used to load and process a batch of data, it takes the batch index as argument.\n",
        "- The on_epoch_end method is called at the end of each epoch, it is optional and takes no arguments.\n",
        "\n",
        "**Note** In case you have enough memory, it is recommanded to process all your data inside the __init__ method and store the processed data in memory. Avoid heavy preprocessing during training, as it can slow down the training.\n",
        "\n",
        "The new data generator can be used as any other data generator in Keras:\n",
        "```python\n",
        "# Create the data generator\n",
        "train_generator = CustomDataGenerator(x_train, y_train, batch_size=32)\n",
        "validation_generator = CustomDataGenerator(x_val, y_val, batch_size=32)\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=10,\n",
        "    # ... other arguments\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq7iC-j37oJH"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deeplearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}